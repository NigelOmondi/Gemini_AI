# -*- coding: utf-8 -*-
"""Gemini_Project_One.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kskOjesD_IXhh9DJwr238Z2JxdYiQWga
"""

!pip install -q -U google-generativeai

import pathlib
import textwrap

import google.generativeai as genai

# Securely store my API KEY
from google.colab import userdata

from IPython.display import display
from IPython.display import Markdown

# Format the text output
def to_markdown(text):
  text = text.replace('â€¢', '  *')
  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

# access and load my api key
GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')

genai.configure(api_key=GOOGLE_API_KEY)

# List all generative models available to me
for model in genai.list_models():
  if 'generateContent' in model.supported_generation_methods:
    print(model.name)

# Generate text from text inputs

model = genai.GenerativeModel('gemini-pro')

# Commented out IPython magic to ensure Python compatibility.
# %%time
# response = model.generate_content("What is the meaning of life?")

to_markdown(response.text)

# Safety concerns regarding the prompt
response.prompt_feedback

# Candidates are multiple possible responses for a single prompt
response.candidates

# Commented out IPython magic to ensure Python compatibility.
# # To stream responses as they come in, model will return chunks
# %%time
# response = model.generate_content("Tell me about the richest man alive?", stream=True)

for chunk in response:
  print(chunk.text)
  print("_"*80)

response = model.generate_content("What is the universal law?", stream=True)

response.prompt_feedback

# When streaming, some responsee attribuntes work but some do not until iteration is complete
try:
  response.text
except Exception as e:
  print(f'{type(e).__name__}: {e}')

# Generate text from image and text inputs

!curl -o image.jpg https://t0.gstatic.com/licensed-image?q=tbn:ANd9GcQ_Kevbk21QBRy-PgB4kQpS79brbmmEG7m3VOTShAn4PecDU5H5UxrJxE3Dw1JiaG17V88QIol19-3TM2wCHw

import PIL.Image

img = PIL.Image.open('image.jpg')
img

model  = genai.GenerativeModel('gemini-pro-vision')

response = model.generate_content(img)

to_markdown(response.text)

response = model.generate_content(["Write a short and engaging blog post on this picture. It should include a description of the meal in the photo and talk about my journey meal prepping.", img])

to_markdown(response.text)

# Chat conversations - Freeform conversations

model = genai.GenerativeModel('gemini-pro')
chat = model.start_chat(history=[])
chat

# Gemini pro vision is not optimized for multi turn chat

response = chat.send_message("In one sentence, explain how a computer works to a young child")
to_markdown(response.text)

chat.history

# To stream the chat
response = chat.send_message("Explain to me the solar system and big bang theory", stream=True)

for chunk in response:
  print(chunk.text)
  print("_"*80)

for message in chat.history:
  display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))

# Multi turn conversations

model = genai.GenerativeModel('gemini-pro')

messages = [
    {'role': 'user',
     'parts': ["Briefly explain neurogenesis and neuroplasticity"]}
]

response = model.generate_content(messages)

to_markdown(response.text)

# For multi turn conversations, you need to send the whole conversation history with each request
# The API is stateless

messages.append({'role': 'model',
                 'parts': [response.text]})

messages.append({'role': 'user',
                 'parts': ["So what is the notable difference between the two?"]})

response = model.generate_content(messages)

to_markdown(response.text)





